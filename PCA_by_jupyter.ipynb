{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library import step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image as im\n",
    "import cv2\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PCA(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(PCA, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        neutral = []\n",
    "\n",
    "        for i in range(2000):\n",
    "\n",
    "            i += 1\n",
    "            img = im.open(f'celeba/img_align_celeba/image{i}.jpg').convert('L') \n",
    "            \n",
    "            img = img.resize((64,64), im.ANTIALIAS) \n",
    "            \n",
    "            img2 = np.array(img).flatten() \n",
    "                      \n",
    "            neutral.append(img2)\n",
    "\n",
    "        faces_matrix = np.vstack(neutral)\n",
    "        mean_face = np.mean(faces_matrix, axis=0)\n",
    "        # plt.imshow(mean_face.reshape(64,64),cmap='gray'); \n",
    "        # plt.title('Mean Face')\n",
    "        # plt.show()\n",
    "          \n",
    "\n",
    "        faces_norm = faces_matrix - mean_face\n",
    "        faces_norm.shape\n",
    "        \n",
    "        # Calculate covariance matrix\n",
    "        face_cov = np.cov(faces_norm.T) #np.cov expects features as rows and observations as columns, so transposed\n",
    "        face_cov.shape\n",
    "        \n",
    "        eigen_vecs, eigen_vals, _ = np.linalg.svd(face_cov)\n",
    "        img = eigen_vecs[:,1].reshape(64,64)      \n",
    "\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show()\n",
    "        #print(img)\n",
    "        # print(\"eigen_vecs.shape\")\n",
    "        # print(eigen_vecs.shape)\n",
    "\n",
    "        \n",
    "        # mean_array = mean_face.reshape(64,64)\n",
    "        # mean_image = im.fromarray(mean_array.astype('uint8'),'L')\n",
    "        # mean_image = mean_image.convert('L')\n",
    "\n",
    "        weight = faces_norm[0,:].dot(eigen_vecs[:,:1]) # Get PC scores of the images\n",
    "        # print(\"weight.shape\")\n",
    "        # print(weight.shape)\n",
    "\n",
    "        projected_face = weight.dot(eigen_vecs[:,:1].T) # Reconstruct first face in dataset using k PCs\n",
    "        # print(\"projected_face.shape\")\n",
    "        # print(projected_face.shape)\n",
    "        # plt.imshow(projected_face.reshape(64,64),cmap='gray')\n",
    "        # plt.show()\n",
    "        \n",
    "\n",
    "        projected_face = projected_face + mean_face\n",
    "        # print(\"projected_mean_face.shape\")\n",
    "        # print(projected_face.shape)\n",
    "        # print(projected_face)\n",
    "\n",
    "        # plt.imshow(projected_face.reshape(64,64)+mean_face.reshape(64,64),cmap='gray');\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        #self.pca = eigen_vecs[:,1]\n",
    "        self.pca = projected_face\n",
    "\n",
    "        # nv + PCA add @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        ##mul_image = cv2.add(src1, src2)\n",
    "            \n",
    "pca_= PCA(ngpu).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "eigenface = pca_.pca\n",
    "eigenface = torch.Tensor(eigenface)\n",
    "\n",
    "\n",
    "eigenface = eigenface.reshape(1,64,64)\n",
    "eigenface = torch.unsqueeze(eigenface, dim=0)\n",
    "\n",
    "# print(\"eigenface unsqueeze shape\")\n",
    "# print(eigenface.shape)\n",
    "\n",
    "# #eigenface = torch.squeeze(eigenface, dim=0)\n",
    "\n",
    "# print(\"eigenface shape\")\n",
    "print(eigenface.shape)\n",
    "\n",
    "\n",
    "#w, h =64,64\n",
    "\n",
    "# #src2 = np.stack([src1, src1, src1], axis=1)\n",
    "#src2 = torch.Tensor(src1)\n",
    "# print(\" src shape\")\n",
    "# print(src1.shape)\n",
    "\n",
    "fixed_noise = torch.randn(1, 100, 1, 1, device=device)\n",
    "#print(\"fixed_noise shape\")\n",
    "#print(fixed_noise.shape)\n",
    "#print(fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\workspace\\DCGAN\\PCA_DCGAN\\PCA_by_jupyter.ipynb 셀 10\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/DCGAN/PCA_DCGAN/PCA_by_jupyter.ipynb#ch0000012?line=45'>46</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain(\u001b[39minput\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/DCGAN/PCA_DCGAN/PCA_by_jupyter.ipynb#ch0000012?line=47'>48</a>\u001b[0m pca_conv\u001b[39m=\u001b[39m PCAConv(ngpu)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/workspace/DCGAN/PCA_DCGAN/PCA_by_jupyter.ipynb#ch0000012?line=48'>49</a>\u001b[0m en \u001b[39m=\u001b[39m pca_conv(eigenface)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/DCGAN/PCA_DCGAN/PCA_by_jupyter.ipynb#ch0000012?line=50'>51</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39men.shape\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/DCGAN/PCA_DCGAN/PCA_by_jupyter.ipynb#ch0000012?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(en\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\VIM_PC\\anaconda3\\envs\\GAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\workspace\\DCGAN\\PCA_DCGAN\\PCA_by_jupyter.ipynb 셀 10\u001b[0m in \u001b[0;36mPCAConv.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/DCGAN/PCA_DCGAN/PCA_by_jupyter.ipynb#ch0000012?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/workspace/DCGAN/PCA_DCGAN/PCA_by_jupyter.ipynb#ch0000012?line=45'>46</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\VIM_PC\\anaconda3\\envs\\GAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\VIM_PC\\anaconda3\\envs\\GAN\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\VIM_PC\\anaconda3\\envs\\GAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\VIM_PC\\anaconda3\\envs\\GAN\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\VIM_PC\\anaconda3\\envs\\GAN\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "class PCAConv(nn.Module):\n",
    "    def __init__(self,npgu):\n",
    "        super(PCAConv, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # nn.ReLU(nn.MaxPool2d(nn.Conv2d(1, 32, 3,1,1, bias=False),2)),\n",
    "            # nn.ReLU(nn.MaxPool2d(nn.Conv2d(32, 64, 3,1,1, bias=False),2)),\n",
    "            # nn.ReLU(nn.MaxPool2d(nn.Conv2d(64, 98, 3,1,1, bias=False),2)),\n",
    "            # nn.ReLU(nn.MaxPool2d(nn.Conv2d(98, 100, 3,1,1, bias=False),2)),\n",
    "            # #nn.Conv1d(1,100,1,1,0,bias=False),\n",
    "            #nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "            nn.Conv2d(1, 32, 4,2,1, bias=False),\n",
    "            nn.Conv2d(32, 64, 4,2,1, bias=False),\n",
    "            nn.Conv2d(64, 96, 4,2,1, bias=False),\n",
    "            nn.Conv2d(96, 100, 4,2,1, bias=False),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "# ############################### 1st\n",
    "#             # input is (nc) x 64 x 64\n",
    "#             nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
    "#             #nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf) x 32 x 32\n",
    "#             nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
    "#             #nn.BatchNorm2d(64 * 2),\n",
    "#             #nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*2) x 16 x 16\n",
    "#             nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
    "#             # nn.BatchNorm2d(64 * 4),\n",
    "#             # nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # # state size. (ndf*4) x 8 x 8\n",
    "#             nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
    "#             # nn.BatchNorm2d(64 * 8),\n",
    "#             # nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # # state size. (ndf*8) x 4 x 4\n",
    "\n",
    "#             #nn.Conv2d(64 * 8, 100, 1, 1, 1, bias=False),\n",
    "#             nn.Conv2d(64 * 8, 64*16, 4, 2, 1, bias=False),\n",
    "#             #nn.Sigmoid()\n",
    "#             nn.Conv2d(64*16, 100, 1, 1,1 ,bias=False),\n",
    "\n",
    "############################### 1st\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "pca_conv= PCAConv(ngpu).to(device)\n",
    "en = pca_conv(eigenface)\n",
    "\n",
    "print(\"en.shape\")\n",
    "print(en.shape)\n",
    "print(en)\n",
    "\n",
    "# img = eigenface.reshape(64,64)      \n",
    "# plt.imshow(img, cmap='gray')\n",
    "# plt.show()\n",
    "\n",
    "nnz = en.clone().detach()\n",
    "\n",
    "print(\"nnz.shape\")\n",
    "print(nnz.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('GAN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27188a68af24359f716f42331a05ac5c5dd63b6f781f49d2f8f4a4081f86e482"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
